# GNT-Arai-2.31-Anne-Jungers
work on Python
# challenge-classification
# Comparison of classifiers

- Repository: `challenge-classification`
- Type of Challenge: `Learning`
- Duration: `4 days`
- Deadline: `dd/mm/yy H:i AM/PM`
- Team challenge : `team` (4)

## Learning Objectives

You will learn to implement different classification algorithms in Python.
At the end of the challenge, you will be able to:

- Choose the most appropriate algorithm, depending on the problem.
- Know the know-how, way to implement and logic behind most common classifiers.
- Manipulate different types of data.

## The Mission

You're part of a team of sweaty mechanics, working with machines all day. You're not mechanically inclined though, you're hired to make an **automated bearing testing system**. Your colleagues made the testing machine, you're here to process the data.
A client has asked you to make a model to use in an scheduled maintenance system. A sample of the bearings in use of their new-fangled machine would be tested, and your model would predict whether a bearing is **faulty or **not**.

The [dataset](https://www.kaggle.com/isaienkov/bearing-classification?select=bearing_signals.csv) is quite hefty, so you'd do well to discuss with your team while it downloads.

### Must-have features

- Fit at least one classification model on the data.
- Have a clean dataset to work with.
- Your code should be commented.
- Your code should be cleaned of any commented unused code.
- Your readme file contains relevant visualizations.
- Each function or class has to contain a docstring formatted like this:

```python
def add(number_one: int, number_two: int) -> int:
    """
    Function that will perform the add operation between two numbers (in params).

    :param number_one: An int that will be added to the second parameter.
    :param number_two: An int that will be added to the fist parameter.
    :return: An int that is the result of the two params being added to each other.
    """
    result = number_one + number_two
    return result
```

### Nice-to-have features

- Perform a grid search to tune a models' parameters.
- test multiple classification models and compare the performance.
- Show the comparison of your models' evaluation metrics.

## Deliverables

1. Publish your source code on the GitHub repository.
2. The deployment link, if applicable.
3. Pimp up the README file:
   - Description
   - Installation
   - Usage
   - (Visuals)
   - (Contributors)
   - (Timeline)
   - (Personal situation)

## Miscellaneous

- A typical classification project has the following timeline:
  - Data gathering
  - Data preprocessing
  - Choose a model
  - Training
  - Evaluation of the model
  - Tuning of hyper-parameters
  - Prediction
  - (Deployment)

### Steps

1. Create the repository.
2. Study the request (What & Why ?)
3. Identify technical challenges (How ?)
4. Implement different classifiers.
5. Compare the results to find the more convenient algorithm.

## Evaluation criteria

| Criteria       | Indicator                                                               | Yes/No |
| -------------- | ----------------------------------------------------------------------- | ------ |
| 1. Is complete | At least 1 classifier was used.                             |        |
|                | Code re-use was limited using functions and classes.                    |        |
|                | There is a published GitHub repo with those.                            |        |
| 2. Is correct  | It recognized correctly the test data.                                  |        |
|                | multiple models were compared between each other and a conclusion was drawn. |        |
| 3. Is clean    | There is good documentation in the project.                             |        |
|                | The code is formatted nicely.                                           |        |

## A final note of encouragement

![You've got this!](https://media.giphy.com/media/idUgpCC6XLoK0ZNZ2o/giphy.gif)

