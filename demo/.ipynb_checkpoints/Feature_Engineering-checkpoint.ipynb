{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432bfcad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039959c9",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/feature-engineering-for-machine-learning-3a5e293a5114  \n",
    "List of Techniques\n",
    "1.Imputation\n",
    "2.Handling Outliers\n",
    "3.Binning\n",
    "4.Log Transform\n",
    "5.One-Hot Encoding\n",
    "6.Grouping Operations\n",
    "7.Feature Split\n",
    "8.Scaling\n",
    "9.Extracting Date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b795d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.7\n",
    "#Dropping columns with missing value rate higher than threshold\n",
    "data = data[data.columns[data.isnull().mean() < threshold]]\n",
    "\n",
    "#Dropping rows with missing value rate higher than threshold\n",
    "data = data.loc[data.isnull().mean(axis=1) < threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e364e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Numerical Imputation\n",
    "\n",
    "#Filling all missing values with 0\n",
    "data = data.fillna(0)\n",
    "#Filling missing values with medians of the columns\n",
    "data = data.fillna(data.median())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbe5673",
   "metadata": {},
   "source": [
    "#Categorical Imputation\n",
    "\n",
    "Replacing the missing values with the maximum occurred value in a column is a good option for handling categorical columns. But if you think the values in the column are distributed uniformly and there is not a dominant value, imputing a category like “Other” might be more sensible, because in such a case, your imputation is likely to converge a random selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c700b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Max fill function for categorical columns\n",
    "data['column_name'].fillna(data['column_name'].value_counts()\n",
    ".idxmax(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40b8b4f",
   "metadata": {},
   "source": [
    "2.Handling Outliers\n",
    "\n",
    "Outlier Detection with Standard Deviation\n",
    "If a value has a distance to the average higher than x * standard deviation, it can be assumed as an outlier. Then what x should be?\n",
    "There is no trivial solution for x, but usually, a value between 2 and 4 seems practical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9de133",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping the outlier rows with standard deviation\n",
    "factor = 3\n",
    "upper_lim = data['column'].mean () + data['column'].std () * factor\n",
    "lower_lim = data['column'].mean () - data['column'].std () * factor\n",
    "\n",
    "data = data[(data['column'] < upper_lim) & (data['column'] > lower_lim)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c975fb",
   "metadata": {},
   "source": [
    "In addition, z-score can be used instead of the formula above. Z-score (or standard score) standardizes the distance between a value and the mean using the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702b78da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Outlier Detection with Percentiles\n",
    "#Dropping the outlier rows with Percentiles\n",
    "upper_lim = data['column'].quantile(.95)\n",
    "lower_lim = data['column'].quantile(.05)\n",
    "\n",
    "data = data[(data['column'] < upper_lim) & (data['column'] > lower_lim)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee576883",
   "metadata": {},
   "outputs": [],
   "source": [
    "#An Outlier Dilemma: Drop or Cap\n",
    "#Capping the outlier rows with Percentiles\n",
    "upper_lim = data['column'].quantile(.95)\n",
    "lower_lim = data['column'].quantile(.05)\n",
    "data.loc[(df[column] > upper_lim),column] = upper_lim\n",
    "data.loc[(df[column] < lower_lim),column] = lower_lim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718839c4",
   "metadata": {},
   "source": [
    "# 3.Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8b57a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Binning can be applied on both categorical and numerical data:\n",
    "#Numerical Binning Example\n",
    "Value      Bin       \n",
    "0-30   ->  Low       \n",
    "31-70  ->  Mid       \n",
    "71-100 ->  High\n",
    "#Categorical Binning Example\n",
    "Value      Bin       \n",
    "Spain  ->  Europe      \n",
    "Italy  ->  Europe       \n",
    "Chile  ->  South America\n",
    "Brazil ->  South America\n",
    "\n",
    "\n",
    "#Numerical Binning Example\n",
    "data['bin'] = pd.cut(data['value'], bins=[0,30,70,100], labels=[\"Low\", \"Mid\", \"High\"])\n",
    "   value   bin\n",
    "0      2   Low\n",
    "1     45   Mid\n",
    "2      7   Low\n",
    "3     85  High\n",
    "4     28   Low\n",
    "#Categorical Binning Example\n",
    "     Country\n",
    "0      Spain\n",
    "1      Chile\n",
    "2  Australia\n",
    "3      Italy\n",
    "4     Brazil\n",
    "conditions = [\n",
    "    data['Country'].str.contains('Spain'),\n",
    "    data['Country'].str.contains('Italy'),\n",
    "    data['Country'].str.contains('Chile'),\n",
    "    data['Country'].str.contains('Brazil')]\n",
    "\n",
    "choices = ['Europe', 'Europe', 'South America', 'South America']\n",
    "\n",
    "data['Continent'] = np.select(conditions, choices, default='Other')\n",
    "     Country      Continent\n",
    "0      Spain         Europe\n",
    "1      Chile  South America\n",
    "2  Australia          Other\n",
    "3      Italy         Europe\n",
    "4     Brazil  South America"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a11e5c",
   "metadata": {},
   "source": [
    "# 4.Log Transform\n",
    "\n",
    "Logarithm transformation (or log transform) is one of the most commonly used mathematical transformations in feature engineering. What are the benefits of log transform:\n",
    "It helps to handle skewed data and after transformation, the distribution becomes more approximate to normal.\n",
    "In most of the cases the magnitude order of the data changes within the range of the data. For instance, the difference between ages 15 and 20 is not equal to the ages 65 and 70. In terms of years, yes, they are identical, but for all other aspects, 5 years of difference in young ages mean a higher magnitude difference. This type of data comes from a multiplicative process and log transform normalizes the magnitude differences like that.\n",
    "It also decreases the effect of the outliers, due to the normalization of magnitude differences and the model become more robust."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71a768b",
   "metadata": {},
   "source": [
    "A critical note: The data you apply log transform must have only positive values, otherwise you receive an error. Also, you can add 1 to your data before transform it. Thus, you ensure the output of the transformation to be positive.\n",
    "Log(x+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32943b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Log Transform Example\n",
    "data = pd.DataFrame({'value':[2,45, -23, 85, 28, 2, 35, -12]})\n",
    "data['log+1'] = (data['value']+1).transform(np.log)\n",
    "#Negative Values Handling\n",
    "#Note that the values are different\n",
    "data['log'] = (data['value']-data['value'].min()+1) .transform(np.log)\n",
    "   value  log(x+1)  log(x-min(x)+1)\n",
    "0      2   1.09861          3.25810\n",
    "1     45   3.82864          4.23411\n",
    "2    -23       nan          0.00000\n",
    "3     85   4.45435          4.69135\n",
    "4     28   3.36730          3.95124\n",
    "5      2   1.09861          3.25810\n",
    "6     35   3.58352          4.07754\n",
    "7    -12       nan          2.48491"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e67f63",
   "metadata": {},
   "source": [
    "# 5.One-hot encoding\n",
    "Why One-Hot?: If you have N distinct values in the column, it is enough to map them to N-1 binary columns, because the missing value can be deducted from other columns. If all the columns in our hand are equal to 0, the missing value must be equal to 1. This is the reason why it is called as one-hot encoding. However, I will give an example using the get_dummies function of Pandas. This function maps all values in a column to multiple columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f685ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_columns = pd.get_dummies(data['column'])\n",
    "data = data.join(encoded_columns).drop('column', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ef8c30",
   "metadata": {},
   "source": [
    "# 6.Grouping Operations\n",
    "In most machine learning algorithms, every instance is represented by a row in the training dataset, where every column show a different feature of the instance. This kind of data called “Tidy”.\n",
    "Tidy datasets are easy to manipulate, model and visualise, and have a specific structure: each variable is a column, each observation is a row, and each type of observational unit is a table.\n",
    "— Hadley Wickham\n",
    "Datasets such as transactions rarely fit the definition of tidy data above, because of the multiple rows of an instance. In such a case, we group the data by the instances and then every instance is represented by only one row.\n",
    "The key point of group by operations is to decide the aggregation functions of the features. For numerical features, average and sum functions are usually convenient options, whereas for categorical features it more complicated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f603fe",
   "metadata": {},
   "source": [
    "### Categorical Column Grouping\n",
    "\n",
    "I suggest three different ways for aggregating categorical columns:\n",
    "The first option is to select the label with the highest frequency. In other words, this is the max operation for categorical columns, but ordinary max functions generally do not return this value, you need to use a lambda function for this purpose.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cba390b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby('id').agg(lambda x: x.value_counts().index[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b55d1e6",
   "metadata": {},
   "source": [
    "Second option is to make a pivot table. This approach resembles the encoding method in the preceding step with a difference. Instead of binary notation, it can be defined as aggregated functions for the values between grouped and encoded columns. This would be a good option if you aim to go beyond binary flag columns and merge multiple features into aggregated features, which are more informative.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdfa2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pivot table Pandas Example\n",
    "data.pivot_table(index='column_to_group', columns='column_to_encode', values='aggregation_column', aggfunc=np.sum, fill_value = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89ca258",
   "metadata": {},
   "source": [
    "Last categorical grouping option is to apply a group by function after applying one-hot encoding. This method preserves all the data -in the first option you lose some-, and in addition, you transform the encoded column from categorical to numerical in the meantime. You can check the next section for the explanation of numerical column grouping.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "639f7328",
   "metadata": {},
   "source": [
    "#### Numerical Column Grouping\n",
    "Numerical columns are grouped using sum and mean functions in most of the cases. Both can be preferable according to the meaning of the feature. For example, if you want to obtain ratio columns, you can use the average of binary columns. In the same example, sum function can be used to obtain the total count either."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1a0f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sum_cols: List of columns to sum\n",
    "#mean_cols: List of columns to average\n",
    "grouped = data.groupby('column_to_group')\n",
    "\n",
    "sums = grouped[sum_cols].sum().add_suffix('_sum')\n",
    "avgs = grouped[mean_cols].mean().add_suffix('_avg')\n",
    "\n",
    "new_df = pd.concat([sums, avgs], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c66f6e6",
   "metadata": {},
   "source": [
    "#  7.Feature Split\n",
    "\n",
    "Splitting features is a good way to make them useful in terms of machine learning. Most of the time the dataset contains string columns that violates tidy data principles. By extracting the utilizable parts of a column into new features:\n",
    "We enable machine learning algorithms to comprehend them.\n",
    "Make possible to bin and group them.\n",
    "Improve model performance by uncovering potential information.\n",
    "Split function is a good option, however, there is no one way of splitting features. It depends on the characteristics of the column, how to split it. Let’s introduce it with two examples. First, a simple split function for an ordinary name column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e419ad6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.name\n",
    "0  Luther N. Gonzalez\n",
    "1    Charles M. Young\n",
    "2        Terry Lawson\n",
    "3       Kristen White\n",
    "4      Thomas Logsdon\n",
    "#Extracting first names\n",
    "data.name.str.split(\" \").map(lambda x: x[0])\n",
    "0     Luther\n",
    "1    Charles\n",
    "2      Terry\n",
    "3    Kristen\n",
    "4     Thomas\n",
    "#Extracting last names\n",
    "data.name.str.split(\" \").map(lambda x: x[-1])\n",
    "0    Gonzalez\n",
    "1       Young\n",
    "2      Lawson\n",
    "3       White\n",
    "4     Logsdon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c44b739",
   "metadata": {},
   "source": [
    "The example above handles the names longer than two words by taking only the first and last elements and it makes the function robust for corner cases, which should be regarded when manipulating strings like that.\n",
    "Another case for split function is to extract a string part between two chars. The following example shows an implementation of this case by using two split functions in a row.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5914a3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#String extraction example\n",
    "data.title.head()\n",
    "0                      Toy Story (1995)\n",
    "1                        Jumanji (1995)\n",
    "2               Grumpier Old Men (1995)\n",
    "3              Waiting to Exhale (1995)\n",
    "4    Father of the Bride Part II (1995)\n",
    "data.title.str.split(\"(\", n=1, expand=True)[1].str.split(\")\", n=1, expand=True)[0]\n",
    "0    1995\n",
    "1    1995\n",
    "2    1995\n",
    "3    1995\n",
    "4    1995"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e27995e",
   "metadata": {},
   "source": [
    "# 8.Scaling\n",
    "In most cases, the numerical features of the dataset do not have a certain range and they differ from each other. In real life, it is nonsense to expect age and income columns to have the same range. But from the machine learning point of view, how these two columns can be compared?\n",
    "Scaling solves this problem. The continuous features become identical in terms of the range, after a scaling process. This process is not mandatory for many algorithms, but it might be still nice to apply. However, the algorithms based on distance calculations such as k-NN or k-Means need to have scaled continuous features as model input.\n",
    "Basically, there are two common ways of scaling:\n",
    "#### Normalization\n",
    "\n",
    "Normalization (or min-max normalization) scale all values in a fixed range between 0 and 1. This transformation does not change the distribution of the feature and due to the decreased standard deviations, the effects of the outliers increases. Therefore, before normalization, it is recommended to handle the outliers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ff8327",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'value':[2,45, -23, 85, 28, 2, 35, -12]})\n",
    "\n",
    "data['normalized'] = (data['value'] - data['value'].min()) / (data['value'].max() - data['value'].min())\n",
    "   value  normalized\n",
    "0      2        0.23\n",
    "1     45        0.63\n",
    "2    -23        0.00\n",
    "3     85        1.00\n",
    "4     28        0.47\n",
    "5      2        0.23\n",
    "6     35        0.54\n",
    "7    -12        0.10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4d8e8f",
   "metadata": {},
   "source": [
    "#### Standardization\n",
    "Standardization (or z-score normalization) scales the values while taking into account standard deviation. If the standard deviation of features is different, their range also would differ from each other. This reduces the effect of the outliers in the features.\n",
    "In the following formula of standardization, the mean is shown as μ and the standard deviation is shown as σ.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ad730f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'value':[2,45, -23, 85, 28, 2, 35, -12]})\n",
    "\n",
    "data['standardized'] = (data['value'] - data['value'].mean()) / data['value'].std()\n",
    "   value  standardized\n",
    "0      2         -0.52\n",
    "1     45          0.70\n",
    "2    -23         -1.23\n",
    "3     85          1.84\n",
    "4     28          0.22\n",
    "5      2         -0.52\n",
    "6     35          0.42\n",
    "7    -12         -0.92"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e0fe75",
   "metadata": {},
   "source": [
    "# 9.Extracting Date\n",
    "Though date columns usually provide valuable information about the model target, they are neglected as an input or used nonsensically for the machine learning algorithms. It might be the reason for this, that dates can be present in numerous formats, which make it hard to understand by algorithms, even they are simplified to a format like \"01–01–2017\".\n",
    "Building an ordinal relationship between the values is very challenging for a machine learning algorithm if you leave the date columns without manipulation. Here, I suggest three types of preprocessing for dates:\n",
    "Extracting the parts of the date into different columns: Year, month, day, etc.\n",
    "Extracting the time period between the current date and columns in terms of years, months, days, etc.\n",
    "Extracting some specific features from the date: Name of the weekday, Weekend or not, holiday or not, etc.\n",
    "If you transform the date column into the extracted columns like above, the information of them become disclosed and machine learning algorithms can easily understand them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550ed6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "data = pd.DataFrame({'date':\n",
    "['01-01-2017',\n",
    "'04-12-2008',\n",
    "'23-06-1988',\n",
    "'25-08-1999',\n",
    "'20-02-1993',\n",
    "]})\n",
    "\n",
    "#Transform string to date\n",
    "data['date'] = pd.to_datetime(data.date, format=\"%d-%m-%Y\")\n",
    "\n",
    "#Extracting Year\n",
    "data['year'] = data['date'].dt.year\n",
    "\n",
    "#Extracting Month\n",
    "data['month'] = data['date'].dt.month\n",
    "\n",
    "#Extracting passed years since the date\n",
    "data['passed_years'] = date.today().year - data['date'].dt.year\n",
    "\n",
    "#Extracting passed months since the date\n",
    "data['passed_months'] = (date.today().year - data['date'].dt.year) * 12 + date.today().month - data['date'].dt.month\n",
    "\n",
    "#Extracting the weekday name of the date\n",
    "data['day_name'] = data['date'].dt.day_name()\n",
    "        date  year  month  passed_years  passed_months   day_name\n",
    "0 2017-01-01  2017      1             2             26     Sunday\n",
    "1 2008-12-04  2008     12            11            123   Thursday\n",
    "2 1988-06-23  1988      6            31            369   Thursday\n",
    "3 1999-08-25  1999      8            20            235  Wednesday\n",
    "4 1993-02-20  1993      2            26            313   Saturday"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
